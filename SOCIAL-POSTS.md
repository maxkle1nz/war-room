# Social Posts — War Room Launch

*Ready to post. Pick the right one for each platform.*

---

## Twitter/X (pick one)

### Option 1 — The Provocation
```
I built an AI system where the agents are required to disagree with each other.

19 protocols. Mandatory devil's advocate. Every decision must steel-man its opposite.

Same project, same model:
• Before: 7/10
• After: 9.5/10

Free, open source, MIT: github.com/maxkle1nz/war-room
```

### Option 2 — The Problem
```
The #1 problem with AI-assisted work: it agrees with everything you say.

War Room fixes this. A CHAOS agent attacks every decision. Agents must declare what they DON'T know. Via Negativa forces you to list 3 things to remove before adding anything.

Result: 6 features cut, 3 risks caught, timeline corrected from 16 to 27 days.

Free: github.com/maxkle1nz/war-room
```

### Option 3 — The Minimal
```
Made an open-source tool.

It runs your project through specialist AI agents + a mandatory devil's advocate.

Same input produced a 7/10 doc, then a 9.5/10 doc.

The difference: 19 protocols that force AI to think harder, not just agree faster.

War Room: github.com/maxkle1nz/war-room
```

---

## Hacker News

**Title:** `Show HN: War Room – AI agents with a built-in devil's advocate (open source)`

**Body:**
```
I built War Room because AI keeps agreeing with me.

Every multi-agent tool I tried had the same problem: agents collaborate politely,
produce impressive-looking output, and nobody questions whether the assumptions
are wrong.

War Room forces structured disagreement. 19 protocols across 4 pillars make
agents declare what they don't know, steel-man the opposite decision, list
things to remove before adding, and stress-test every conclusion.

The key: a mandatory CHAOS agent that attacks every decision and rates them
SURVIVES, WOUNDED, or KILLED.

I tested it on a macOS app project. Standard multi-agent session: 7/10 PRD.
Same input through War Room: 9.5/10. It cut 6 features, caught a critical
auto-update gap, corrected the timeline by 50%, and found that 4 of 5 failure
scenarios came from one dependency nobody else flagged.

Then I ran it on a music career decision (sign vs stay independent) to prove
it works beyond software. Both examples are in the repo.

Free. MIT license. Works with OpenClaw or any multi-agent setup.

github.com/maxkle1nz/war-room
```

---

## Reddit (/r/ChatGPT or /r/LocalLLaMA)

**Title:** `I made AI argue with itself. Here's what happened.`

**Body:**
```
After months of using AI for complex decisions, I noticed the same pattern:
I describe what I want, AI says "great idea!", and I ship something that
falls apart when reality shows up.

The problem isn't the model. It's the protocol.

So I built War Room — a system where multiple AI agents work on your problem,
but one of them (the CHAOS agent) is REQUIRED to attack every decision.

Other protocols force agents to:
- State the opposite of their decision and argue FOR it
- Declare what they DON'T know before analyzing
- List 3 things to REMOVE before adding anything
- Explain how their work fails in production

I tested it side-by-side. Same project, same model. The version without
protocols: over-scoped, optimistic timeline, missed critical risks.
The version with protocols: cut 6 features, caught 3 risks, honest timeline.

It's free and open source (MIT): github.com/maxkle1nz/war-room

Has examples for both software and non-software decisions.
```

---

## LinkedIn / Discord (longer format)

```
Why I Built a System Where AI Argues With Itself

I've been building with AI for a while. The pattern is always the same:
you describe a project, AI says "great idea!", and produces something
that looks impressive but falls apart when reality shows up.

The problem isn't the model. It's the protocol.

When a human team works on a complex problem, the magic happens in the
friction. The engineer who says "that timeline is unrealistic." The
security person who says "you haven't modeled this threat." The person
who asks "do we even need this feature?"

AI doesn't do this by default. It optimizes for agreement.

So I built War Room — an open-source system that forces structured
disagreement through 19 decision protocols.

The key innovation: a CHAOS agent that attacks every decision, rating
them SURVIVES, WOUNDED, or KILLED. Plus counter-proposals nobody else
considered.

I tested it on a real project. Same input, same model:
• Standard: 10 features, "16 days" timeline, 0 risks caught
• War Room: 8 features, "27 days" honest timeline, 3 critical risks,
  a counter-proposal that ships 80% of the value in 3 days

Then I ran it on a music industry decision to prove it works
beyond software.

Free. MIT license. github.com/maxkle1nz/war-room

"O melhor conhecimento é aquele que é passado adiante."
```
